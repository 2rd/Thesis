{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import scipy\n",
    "import sklearn\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def load_item_dataset(path_to_dataset):\n",
    "    dataset = []\n",
    "    for movie_features in os.listdir(path_to_dataset):\n",
    "        movie = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "        movie_id = movie_features.lstrip('0')\n",
    "        movie_id = movie_id[:-2]\n",
    "        movie['movie_id'] = movie_id\n",
    "        \n",
    "        infile = open(f\"{path_to_dataset}/{movie_features}\", \"rb\")\n",
    "        movie_data = pickle.load(infile)\n",
    "        infile.close()       \n",
    "        movie['data'] = movie_data\n",
    "        \n",
    "        \n",
    "        \n",
    "        dataset.append(movie)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def load_movielens(path_to_data):\n",
    "    as_df = pd.read_csv(path_to_data)\n",
    "    return as_df\n",
    "\n",
    "\n",
    "def labels_to_string(movie):\n",
    "    movie_words = ''\n",
    "    for frame in movie['data']:\n",
    "        word = frame[0][1]\n",
    "        movie_words += f' {word}'\n",
    "        \n",
    "    return movie_words\n",
    "\n",
    "def labels_to_list(movie):\n",
    "    movie_labels = []\n",
    "    for frame in movie['data']:\n",
    "        word = frame[0][1].replace('_', '')\n",
    "        movie_labels.append(word)\n",
    "        \n",
    "    return movie_labels\n",
    "\n",
    "def words_to_vocab(words):\n",
    "    vocab = {}\n",
    "    for word in words:\n",
    "        if word not in vocab.keys():\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_all_words(label_dataset):    \n",
    "    all_words = []\n",
    "    for movie in label_dataset:\n",
    "        movie_words = labels_to_string(movie)            \n",
    "        all_words.append(movie_words)\n",
    "        \n",
    "    return all_words\n",
    "\n",
    "def create_label_features(movie):\n",
    "  label_dict = dict()\n",
    "  for frame in movie['data']:\n",
    "    word = frame[0][1]\n",
    "    if word not in label_dict.keys():\n",
    "      label_dict[word] = np.array([frame[0][2]])\n",
    "    else:\n",
    "      label_dict[word] = np.append(label_dict[word], frame[0][2])\n",
    "\n",
    "  for word in label_dict.keys():\n",
    "    label_dict[word] = label_dict[word].mean()\n",
    "  \n",
    "  return {'movie_id':movie['movie_id'], 'labels':label_dict}\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "#features_dataset = load_item_dataset('../data/features')\n",
    "labels_dataset = load_item_dataset('D:/Masterprosjekt/Labels')\n",
    "ratings_dataset = load_movielens('../data/ml-20m/ratings.csv')\n",
    "ratings_dataset = ratings_dataset.drop(columns=['timestamp'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "pickle.dump(labels_dataset, (open('../data/labels_dataset_raw.p', 'wb')))\n",
    "pickle.dump(ratings_dataset, (open('../data/ratings_dataset_raw.p', 'wb')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Labels confidence\n",
    "labels_dataset = load_item_dataset('D:/Masterprosjekt/Labels')\n",
    "labels_dataset = map(create_label_features, labels_dataset)\n",
    "\n",
    "labels_dataset = pd.DataFrame.from_records(labels_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/26734 [00:00<?, ?it/s]",
      "\r  0%|          | 1/26734 [00:05<41:22:04,  5.57s/it]",
      "\r  0%|          | 2/26734 [00:17<55:32:21,  7.48s/it]",
      "\r  0%|          | 3/26734 [00:26<57:45:21,  7.78s/it]",
      "\r  0%|          | 4/26734 [00:33<56:39:45,  7.63s/it]",
      "\r  0%|          | 5/26734 [00:39<54:27:35,  7.33s/it]",
      "\r  0%|          | 6/26734 [00:45<49:32:04,  6.67s/it]",
      "\r  0%|          | 7/26734 [00:50<46:31:56,  6.27s/it]",
      "\r  0%|          | 8/26734 [00:56<45:58:16,  6.19s/it]",
      "\r  0%|          | 9/26734 [01:01<43:49:42,  5.90s/it]",
      "\r  0%|          | 10/26734 [01:06<41:50:50,  5.64s/it]",
      "\r  0%|          | 11/26734 [01:14<46:00:43,  6.20s/it]",
      "\r  0%|          | 12/26734 [01:19<45:04:47,  6.07s/it]",
      "\r  0%|          | 13/26734 [01:25<42:59:06,  5.79s/it]",
      "\r  0%|          | 13/26734 [01:27<50:04:40,  6.75s/it]",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3f31567dd472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mindexNames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mratings_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mratings_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movieId'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mratings_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexNames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Feature extraction\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4162\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4163\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4164\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4165\u001b[0m         )\n\u001b[0;32m   4166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Feature extraction\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3876\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3877\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3878\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3880\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Feature extraction\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3904\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3906\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Feature extraction\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mis_unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mhas\u001b[0m \u001b[0munique\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m-> 1645\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "# Remove interactions (ratings) for all movies not in the labels dataset\n",
    "\n",
    "unique = ratings_dataset.movieId.unique()\n",
    "unique = list(map(str, unique))\n",
    "# Find all movies not present in the ratings dataset\n",
    "for movie in labels_dataset:\n",
    "    if movie['movie_id'] in unique:\n",
    "        unique.remove(movie['movie_id'])\n",
    "\n",
    "# Remove all movies not present in the ratings dataset\n",
    "for value in tqdm(unique):\n",
    "    indexNames = ratings_dataset[ratings_dataset['movieId'] == int(value)].index\n",
    "    ratings_dataset.drop(indexNames, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Remove all movies not present in labels dataset\n",
    "\n",
    "unique = ratings_dataset.movieId.unique()\n",
    "\n",
    "unique = set(map(str, unique))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "3254\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(len(labels_dataset))\n",
    "for movie in labels_dataset:\n",
    "    if movie['movie_id'] not in unique:\n",
    "        print(movie['movie_id'])\n",
    "        labels_dataset.remove(movie)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "full_vocab = {}\n",
    "\n",
    "\n",
    "for movie in labels_dataset:\n",
    "    vocab = words_to_vocab(labels_to_list(movie))\n",
    "    movie['corpus'] = vocab\n",
    "\n",
    "\n",
    "for corpus in (movie['corpus'] for movie in labels_dataset):\n",
    "    for word in corpus.keys():\n",
    "        if word not in full_vocab.keys():\n",
    "            full_vocab[word] = 1\n",
    "        else: \n",
    "            full_vocab[word] += corpus[word]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Vectorize labels\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(get_all_words(labels_dataset)).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Normalize vectors\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Load list of vectors for each movie into labels dataset in column named 'labels'\n",
    "labels_dataset['labels'] = labels_dataset['labels'].astype('object')\n",
    "i = 0\n",
    "for movie in X_scaled:\n",
    "    labels_dataset.at[i , 'labels'] = movie.tolist()\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Load vectors into labels dataset where each possible label has its own column\n",
    "\n",
    "i = 0\n",
    "for movie in X_scaled:\n",
    "    y = 0\n",
    "    for label in movie:\n",
    "        labels_dataset[i][f'{y}'] = label\n",
    "        y += 1\n",
    "    i+=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Convert labels dataset into DataFrame\n",
    "\n",
    "labels_dataset = pd.DataFrame.from_records(labels_dataset)\n",
    "labels_dataset = labels_dataset.drop(columns='data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "for word in full_vocab.keys():\n",
    "    labels_dataset[word] = 0\n",
    "\n",
    "for index, row in labels_dataset.iterrows():\n",
    "    for word in labels_dataset.keys():\n",
    "        if word in row['corpus'].keys():\n",
    "            # labels_dataset.at[index, word] = row['corpus'][word]\n",
    "            labels_dataset.at[index, word] = 1\n",
    "            \n",
    "labels_dataset = labels_dataset.drop(columns='corpus')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "pickle.dump(labels_dataset, (open('../data/labels_dataset.p', 'wb')))\n",
    "pickle.dump(ratings_dataset, (open('../data/ratings_dataset.p', 'wb')))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "infile = open(\"../data/labels_dataset.p\", \"rb\")\n",
    "labels_dataset = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"../data/ratings_dataset.p\", \"rb\")\n",
    "ratings_dataset = pickle.load(infile)\n",
    "infile.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Split ratings dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(ratings_dataset, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# Create lightfm Dataset\n",
    "# Create ID mappings between users and movies\n",
    "\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit((row['userId'] for index, row in ratings_dataset.iterrows()), \n",
    "            (row['movieId'] for index, row in ratings_dataset.iterrows()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(3254, 985)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(labels_dataset.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Num users: 610, num_items 3254.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "num_users, num_items = dataset.interactions_shape()\n",
    "print('Num users: {}, num_items {}.'.format(num_users, num_items))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "columns = labels_dataset.columns\n",
    "possible_features = []\n",
    "for i in range(len(columns)):\n",
    "    if(columns[i]) != 'movie_id':\n",
    "        possible_features.append(f'{columns[i]}:0')\n",
    "        possible_features.append(f'{columns[i]}:1')\n",
    "        \n",
    "dataset.fit_partial(item_features = possible_features) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-587daf19229d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset.fit_partial(\n\u001b[0;32m      5\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'movie_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mitem_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         )\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Feature extraction\\lib\\site-packages\\lightfm\\data.py\u001b[0m in \u001b[0;36mfit_partial\u001b[1;34m(self, users, items, user_features, item_features)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mitem_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem_feature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_feature_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_feature_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_unpack_datum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ],
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error"
    }
   ],
   "source": [
    "# Add item features to the lightfm Dataset when labels are given as a \n",
    "# list for each movie in labels_dataset['labels']\n",
    "\n",
    "dataset.fit_partial(\n",
    "            item_features = (row['labels'] for index, row in labels_dataset.iterrows())\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Add item features to the lightfm Dataset when each possible label \n",
    "# has its own column in labels_dataset\n",
    "\n",
    "for column in labels_dataset:\n",
    "    if column != 'movie_id': \n",
    "        dataset.fit_partial(\n",
    "            items = (row['movie_id'] for index, row in labels_dataset.iterrows()),\n",
    "            item_features = (row[str(column)] for index, row in labels_dataset.iterrows())\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<610x3254 sparse matrix of type '<class 'numpy.int32'>'\n\twith 43381 stored elements in COOrdinate format>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Build the interaction matrix for the lightfm Dataset\n",
    "\n",
    "(interactions, weights) = dataset.build_interactions(((row['userId'], row['movieId'], row['rating']) for index, row in ratings_dataset.iterrows()))\n",
    "\n",
    "print(repr(interactions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "pickle.dump(dataset, (open('../data/lightfm_dataset.p', 'wb')))\n",
    "pickle.dump(interactions, (open('../data/lightfm_interactions.p', 'wb')))\n",
    "pickle.dump(weights, (open('../data/lightfm_weights.p', 'wb')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "infile = open(\"../data/lightfm_dataset.p\", \"rb\")\n",
    "dataset = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"../data/lightfm_interactions.p\", \"rb\")\n",
    "interactions = pickle.load(infile)\n",
    "infile.close()\n",
    "infile = open(\"../data/lightfm_weights.p\", \"rb\")\n",
    "weights = pickle.load(infile)\n",
    "infile.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the item features matrix for the lightfm Dataset when labels are given as a \n",
    "# list for each movie in labels_dataset['labels']\n",
    "\n",
    "item_features = dataset.build_item_features(((row['movie_id'], row['labels'])\n",
    "                                              for index, row in labels_dataset.iterrows()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "columns = labels_dataset.columns\n",
    "item_tuples = []\n",
    "for i, r in labels_dataset.iterrows():\n",
    "    features = []\n",
    "    movie_id = r['movie_id']\n",
    "    for y in range(len(columns)):\n",
    "        if(columns[y]) != 'movie_id':\n",
    "            features.append(f'{columns[y]}:{labels_dataset.at[i, columns[y]]}')\n",
    "    \n",
    "    item_tuples.append((int(movie_id), features))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Build the item features matrix for the lightfm Dataset when each possible label \n",
    "# has its own column in labels_dataset \n",
    "\n",
    "item_features = dataset.build_item_features(item_tuples, normalize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "pickle.dump(item_features, open('../data/item_features.p', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "matrix([[1., 0., 0., ..., 0., 1., 0.],\n        [0., 1., 0., ..., 0., 1., 0.],\n        [0., 0., 1., ..., 0., 1., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 1., 0.],\n        [0., 0., 0., ..., 0., 1., 0.],\n        [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 109
    }
   ],
   "source": [
    "item_features.todense()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "interactions_csr = interactions.tocsr()\n",
    "\n",
    "train, test = train_test_split(interactions_csr, test_size=0.2)\n",
    "train = train.tocoo()\n",
    "test = test.tocoo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from lightfm.cross_validation import random_train_test_split\n",
    "\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Wall time: 2.68 s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "# Set the number of threads; you can increase this\n",
    "# ify you have more physical cores available.\n",
    "NUM_THREADS = 2\n",
    "NUM_COMPONENTS = 30\n",
    "NUM_EPOCHS = 3\n",
    "ITEM_ALPHA = 1e-6\n",
    "\n",
    "model = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                no_components=NUM_COMPONENTS)\n",
    "\n",
    "%time model = model.fit(train, epochs=NUM_EPOCHS, num_threads=NUM_THREADS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Rating based train AUC: 0.9324894\n",
      "Rating based train precision: 0.40786886\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_auc = auc_score(model, train, num_threads=NUM_THREADS).mean()\n",
    "print('Rating based train AUC: %s' % train_auc)\n",
    "train_precision = precision_at_k(model, \n",
    "                                 train,\n",
    "                                 num_threads=NUM_THREADS).mean()\n",
    "print('Rating based train precision: %s' % train_precision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Rating based test AUC: 0.9090621\n",
      "Rating based test precision: 0.17487438\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "test_auc = auc_score(model, test, train_interactions=train, num_threads=NUM_THREADS).mean()\n",
    "print('Rating based test AUC: %s' % test_auc)\n",
    "test_precision = precision_at_k(model, \n",
    "                                 test, \n",
    "                                 train_interactions=train,\n",
    "                                 num_threads=NUM_THREADS).mean()\n",
    "print('Rating based test precision: %s' % test_precision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Rating based test AUC: 0.89363015\nRating based test precision: 0.14288108\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Set biases to zero\n",
    "model.item_biases *= 0.0\n",
    "\n",
    "test_auc = auc_score(model, test, num_threads=NUM_THREADS).mean()\n",
    "test_precision = precision_at_k(model, \n",
    "                                 test, \n",
    "                                 train_interactions=train,\n",
    "                                 num_threads=NUM_THREADS).mean()\n",
    "print('Rating based test AUC: %s' % test_auc)\n",
    "print('Rating based test precision: %s' %test_precision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Define a new model instance\n",
    "model = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                no_components=NUM_COMPONENTS)\n",
    "\n",
    "# Fit the hybrid model. Note that this time, we pass\n",
    "# in the item features matrix.\n",
    "model = model.fit(train,\n",
    "                item_features=item_features,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                num_threads=NUM_THREADS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Hybrid training set AUC: 0.85032\n",
      "Hybrid training set precision: 0.18606557\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Don't forget the pass in the item features again!\n",
    "train_auc = auc_score(model,\n",
    "                      train,\n",
    "                      item_features=item_features,\n",
    "                      num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid training set AUC: %s' % train_auc)\n",
    "train_precision = precision_at_k(model, \n",
    "                                 train, \n",
    "                                 item_features=item_features,\n",
    "                                 num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid training set precision: %s' % train_precision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Hybrid test set AUC: 0.75393724\n",
      "Hybrid test set precision: 0.054606363\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "test_auc = auc_score(model,\n",
    "                    test,\n",
    "                    train_interactions=train,\n",
    "                    item_features=item_features,\n",
    "                    num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test set AUC: %s' % test_auc)\n",
    "test_precision = precision_at_k(model, \n",
    "                                 test, \n",
    "                                 train_interactions=train,\n",
    "                                 item_features=item_features,\n",
    "                                 num_threads=NUM_THREADS,\n",
    "                                k=10).mean()\n",
    "print('Hybrid test set precision: %s' % test_precision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_precision = precision_at_k(model, train, item_features=item_features, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, item_features=item_features, k=10).mean()\n",
    "\n",
    "train_auc = auc_score(model, train, item_features=item_features).mean()\n",
    "test_auc = auc_score(model, test, item_features=item_features).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "infile = open('Output.txt', 'w')\n",
    "for columns in labels_dataset.columns:\n",
    "    if columns != 'movie_id':\n",
    "        infile.write(rf\"row['{columns}'], \")\n",
    "infile.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['movie_id', 'bowtie', 'spotlight', 'vendingmachine', 'firescreen',\n       'sax', 'seatbelt', 'matchstick', 'planetarium', 'abaya',\n       ...\n       'carbonara', 'manholecover', 'jay', 'tigerbeetle', 'cricket',\n       'soupbowl', 'pizza', 'fig', 'gartersnake', 'ruffedgrouse'],\n      dtype='object', length=985)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 56
    }
   ],
   "source": [
    "infile = open(\"../data/labels_dataset.p\", \"rb\")\n",
    "test = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "test.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "local-venv",
   "language": "python",
   "display_name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}